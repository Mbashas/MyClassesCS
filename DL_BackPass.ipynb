{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backward_propagation(cache, target, W1, W2, learning_rate=0.1, activation_function='linear'):\n",
    "    \"\"\"\n",
    "    Perform backpropagation through a 2-layer neural network\n",
    "    \n",
    "    Parameters:\n",
    "    cache -- dictionary containing values from forward propagation\n",
    "    target -- target values\n",
    "    W1, W2 -- weight matrices\n",
    "    learning_rate -- learning rate for gradient descent\n",
    "    activation_function -- 'linear' or 'sigmoid'\n",
    "    \n",
    "    Returns:\n",
    "    Updated weights and biases W1, b1, W2, b2\n",
    "    gradients -- dictionary containing the gradients\n",
    "    \"\"\"\n",
    "    # Retrieve values from cache\n",
    "    X = cache[\"X\"]\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    Z1 = cache[\"Z1\"]\n",
    "    \n",
    "    # Number of examples\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Calculate error at output\n",
    "    error = A2 - target\n",
    "    \n",
    "    # Backpropagation for the second layer\n",
    "    if activation_function == 'linear':\n",
    "        dZ2 = error  # Derivative of linear function is 1\n",
    "    elif activation_function == 'sigmoid':\n",
    "        dZ2 = error * A2 * (1 - A2)  # Derivative of sigmoid: f'(x) = f(x)(1-f(x))\n",
    "    \n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    # Backpropagation for the first layer\n",
    "    if activation_function == 'linear':\n",
    "        dZ1 = np.dot(W2.T, dZ2)  # Derivative of linear function is 1\n",
    "    elif activation_function == 'sigmoid':\n",
    "        dA1 = np.dot(W2.T, dZ2)\n",
    "        dZ1 = dA1 * A1 * (1 - A1)  # Derivative of sigmoid\n",
    "    \n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    # Update weights and biases using gradient descent\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    # Store gradients for inspection\n",
    "    gradients = {\n",
    "        \"dW1\": dW1,\n",
    "        \"db1\": db1,\n",
    "        \"dW2\": dW2,\n",
    "        \"db2\": db2\n",
    "    }\n",
    "    \n",
    "    return W1, b1, W2, b2, gradients"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
