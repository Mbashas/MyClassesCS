{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 117.36915667029264\n",
      "Cost after iteration 100: nan\n",
      "Cost after iteration 200: nan\n",
      "Cost after iteration 300: nan\n",
      "Cost after iteration 400: nan\n",
      "Cost after iteration 500: nan\n",
      "Cost after iteration 600: nan\n",
      "Cost after iteration 700: nan\n",
      "Cost after iteration 800: nan\n",
      "Cost after iteration 900: nan\n",
      "Predictions: [[nan nan nan]\n",
      " [nan nan nan]]\n",
      "Targets: [[4 5 6]\n",
      " [7 8 9]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seth\\AppData\\Local\\Temp\\ipykernel_12688\\3244156104.py:96: RuntimeWarning: overflow encountered in square\n",
      "  cost = (1/m) * np.sum(np.square(AL - Y))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_sizes, activation='linear'):\n",
    "        \"\"\"\n",
    "        Initialize a neural network with specified layer sizes\n",
    "        \n",
    "        Parameters:\n",
    "        layer_sizes -- list containing the number of nodes in each layer\n",
    "                      (including input and output layers)\n",
    "        activation -- activation function to use ('linear' or 'sigmoid')\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        self.activation = activation\n",
    "        self.parameters = {}\n",
    "        self.initialize_parameters()\n",
    "        \n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"Initialize weights and biases with random values\"\"\"\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        \n",
    "        for l in range(1, self.num_layers):\n",
    "            # He initialization for better training with deep networks\n",
    "            self.parameters[f'W{l}'] = np.random.randn(self.layer_sizes[l], \n",
    "                                                      self.layer_sizes[l-1]) * np.sqrt(2 / self.layer_sizes[l-1])\n",
    "            self.parameters[f'b{l}'] = np.zeros((self.layer_sizes[l], 1))\n",
    "    \n",
    "    def activation_function(self, Z, derivative=False):\n",
    "        \"\"\"Apply the activation function\"\"\"\n",
    "        if self.activation == 'linear':\n",
    "            if derivative:\n",
    "                return np.ones_like(Z)  # Derivative of linear function is 1\n",
    "            return Z  # Linear function: f(x) = x\n",
    "        \n",
    "        elif self.activation == 'sigmoid':\n",
    "            if derivative:\n",
    "                # Derivative of sigmoid: f'(x) = f(x)(1-f(x))\n",
    "                sigmoid = 1 / (1 + np.exp(-Z))\n",
    "                return sigmoid * (1 - sigmoid)\n",
    "            # Sigmoid function: 1 / (1 + e^(-x))\n",
    "            return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward propagation through the network\n",
    "        \n",
    "        Parameters:\n",
    "        X -- input data, shape (input_size, number_of_examples)\n",
    "        \n",
    "        Returns:\n",
    "        AL -- output of the network\n",
    "        caches -- list of caches containing values needed for backpropagation\n",
    "        \"\"\"\n",
    "        caches = []\n",
    "        A = X\n",
    "        \n",
    "        # Forward propagation through each layer\n",
    "        for l in range(1, self.num_layers):\n",
    "            A_prev = A\n",
    "            W = self.parameters[f'W{l}']\n",
    "            b = self.parameters[f'b{l}']\n",
    "            \n",
    "            # Linear forward\n",
    "            Z = np.dot(W, A_prev) + b\n",
    "            \n",
    "            # Activation\n",
    "            A = self.activation_function(Z)\n",
    "            \n",
    "            # Save values for backpropagation\n",
    "            cache = {\n",
    "                \"A_prev\": A_prev,\n",
    "                \"W\": W,\n",
    "                \"b\": b,\n",
    "                \"Z\": Z,\n",
    "                \"A\": A\n",
    "            }\n",
    "            caches.append(cache)\n",
    "        \n",
    "        return A, caches\n",
    "    \n",
    "    def compute_cost(self, AL, Y):\n",
    "        \"\"\"\n",
    "        Compute the mean squared error cost\n",
    "        \n",
    "        Parameters:\n",
    "        AL -- output of the forward propagation (output of the last activation)\n",
    "        Y -- target values\n",
    "        \n",
    "        Returns:\n",
    "        cost -- mean squared error\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]  # Number of examples\n",
    "        \n",
    "        # Mean squared error cost\n",
    "        cost = (1/m) * np.sum(np.square(AL - Y))\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def backward_propagation(self, Y, caches):\n",
    "        \"\"\"\n",
    "        Perform backward propagation through the network\n",
    "        \n",
    "        Parameters:\n",
    "        Y -- target values\n",
    "        caches -- list of caches from forward propagation\n",
    "        \n",
    "        Returns:\n",
    "        gradients -- dictionary containing gradients\n",
    "        \"\"\"\n",
    "        gradients = {}\n",
    "        m = Y.shape[1]  # Number of examples\n",
    "        L = len(caches)  # Number of layers (excluding input layer)\n",
    "        \n",
    "        # Initialize backpropagation with the output layer\n",
    "        AL = caches[L-1][\"A\"]\n",
    "        \n",
    "        # Derivative of cost with respect to AL\n",
    "        dAL = 2 * (AL - Y) / m\n",
    "        \n",
    "        # Backpropagate through layers\n",
    "        dA_curr = dAL\n",
    "        \n",
    "        for l in reversed(range(L)):\n",
    "            cache = caches[l]\n",
    "            A_prev = cache[\"A_prev\"]\n",
    "            W = cache[\"W\"]\n",
    "            Z = cache[\"Z\"]\n",
    "            \n",
    "            # Compute gradients\n",
    "            dZ = dA_curr * self.activation_function(Z, derivative=True)\n",
    "            dW = np.dot(dZ, A_prev.T)\n",
    "            db = np.sum(dZ, axis=1, keepdims=True)\n",
    "            \n",
    "            if l > 0:\n",
    "                # Propagate to previous layer (not needed for the first layer)\n",
    "                dA_curr = np.dot(W.T, dZ)\n",
    "            \n",
    "            # Store gradients\n",
    "            gradients[f'dW{l+1}'] = dW\n",
    "            gradients[f'db{l+1}'] = db\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def update_parameters(self, gradients, learning_rate):\n",
    "        \"\"\"\n",
    "        Update weights and biases using gradient descent\n",
    "        \n",
    "        Parameters:\n",
    "        gradients -- dictionary containing gradients\n",
    "        learning_rate -- learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        for l in range(1, self.num_layers):\n",
    "            self.parameters[f'W{l}'] -= learning_rate * gradients[f'dW{l}']\n",
    "            self.parameters[f'b{l}'] -= learning_rate * gradients[f'db{l}']\n",
    "    \n",
    "    def train(self, X, Y, num_iterations=1000, learning_rate=0.1, print_cost=False, print_interval=100):\n",
    "        \"\"\"\n",
    "        Train the neural network\n",
    "        \n",
    "        Parameters:\n",
    "        X -- input data, shape (input_size, number_of_examples)\n",
    "        Y -- target values, shape (output_size, number_of_examples)\n",
    "        num_iterations -- number of iterations for training\n",
    "        learning_rate -- learning rate for gradient descent\n",
    "        print_cost -- if True, print the cost every print_interval iterations\n",
    "        print_interval -- interval at which to print costs\n",
    "        \n",
    "        Returns:\n",
    "        costs -- list of costs during training\n",
    "        \"\"\"\n",
    "        costs = []\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            # Forward propagation\n",
    "            AL, caches = self.forward_propagation(X)\n",
    "            \n",
    "            # Compute cost\n",
    "            cost = self.compute_cost(AL, Y)\n",
    "            \n",
    "            # Backward propagation\n",
    "            gradients = self.backward_propagation(Y, caches)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.update_parameters(gradients, learning_rate)\n",
    "            \n",
    "            # Print cost\n",
    "            if print_cost and i % print_interval == 0:\n",
    "                print(f\"Cost after iteration {i}: {cost}\")\n",
    "            \n",
    "            # Record cost\n",
    "            if i % 100 == 0:\n",
    "                costs.append(cost)\n",
    "        \n",
    "        return costs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained network\n",
    "        \n",
    "        Parameters:\n",
    "        X -- input data\n",
    "        \n",
    "        Returns:\n",
    "        predictions -- output of the network\n",
    "        \"\"\"\n",
    "        AL, _ = self.forward_propagation(X)\n",
    "        return AL\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        \"\"\"Return the network parameters\"\"\"\n",
    "        return self.parameters\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Define network architecture [input_size, hidden_size, output_size]\n",
    "    layer_sizes = [2, 3, 2]\n",
    "    \n",
    "    # Create neural network with linear activation\n",
    "    nn = NeuralNetwork(layer_sizes, activation='linear')\n",
    "    \n",
    "    # Create sample data\n",
    "    X = np.array([[1, 2, 3], [4, 5, 6]])  # 2 features, 3 examples\n",
    "    Y = np.array([[4, 5, 6], [7, 8, 9]])  # 2 outputs, 3 examples\n",
    "    \n",
    "    # Train the network\n",
    "    costs = nn.train(X, Y, num_iterations=1000, learning_rate=0.1, print_cost=True)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = nn.predict(X)\n",
    "    print(\"Predictions:\", predictions)\n",
    "    print(\"Targets:\", Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
